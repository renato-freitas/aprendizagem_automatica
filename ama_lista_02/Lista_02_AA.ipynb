{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "escalar_minmax = MinMaxScaler()\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funções\n",
    "def sigmoide(z):\n",
    "  \"\"\"Função logística Sigmóide, y^i = o(wTxi), o(z) = 1/1 + exp(-z)\"\"\"\n",
    "  return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def custo_cross_entropia_binaria(y, y_pred):\n",
    "  \"\"\"Função Custo Cross Entropy, Regressão Logística Binária\"\"\"\n",
    "  sujeira = 1.0e-18 \n",
    "  return -np.mean(y*(np.log(sujeira + y_pred)) + (1 - y)*np.log(sujeira + (1 - y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt(\"./breastcancer.csv\", delimiter=',', skip_header=1)\n",
    "X = dataset[:, :30]\n",
    "X = escalar_minmax.fit_transform(X)\n",
    "y = dataset[:, -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         acurácia,            std,                acurácia classe0,     acuracia clasee1\n",
      "Fold 1 [0.08070175438596491, 0.4868223482635652, 0.054385964912280704, 0.02631578947368421]\n",
      "Fold 2 [0.16666666666666666, 0.4962152850431912, 0.10175438596491229, 0.0649122807017544]\n",
      "Fold 3 [0.2631578947368421, 0.4772445792538752, 0.1631578947368421, 0.1]\n",
      "Fold 4 [0.3614035087719298, 0.4906011036529671, 0.22280701754385968, 0.13859649122807016]\n",
      "Fold 5 [0.4508771929824561, 0.4714045207910317, 0.28771929824561404, 0.16315789473684209]\n",
      "Fold 6 [0.5385964912280701, 0.4868223482635652, 0.3456140350877193, 0.19298245614035087]\n",
      "Fold 7 [0.631578947368421, 0.4772445792538752, 0.4087719298245614, 0.22280701754385962]\n",
      "Fold 8 [0.7157894736842104, 0.4714045207910317, 0.4631578947368421, 0.25263157894736843]\n",
      "Fold 9 [0.8068609022556391, 0.4883855118277623, 0.518515037593985, 0.2883458646616541]\n",
      "Fold 10 [0.8943609022556391, 0.49196347549842545, 0.5720864661654136, 0.32227443609022555]\n"
     ]
    }
   ],
   "source": [
    "epocas = 1000\n",
    "alfa = 0.3\n",
    "# alfa = 0.01\n",
    "custos = []\n",
    "\n",
    "def regressao_logistica(X, y, W):\n",
    "  n = X.shape[0]\n",
    "  for i in range(epocas):\n",
    "      z = X @ W\n",
    "      y_pred = sigmoide(z)\n",
    "      erro = y - y_pred\n",
    "      \n",
    "      W = W + alfa*((X.T @ erro)/n)\n",
    "      custo = custo_cross_entropia_binaria(y, y_pred)\n",
    "      custos.append([i, custo])\n",
    "  return W\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "fold = 0\n",
    "classe = []\n",
    "acuracia = 0.0\n",
    "acuracia_classe1 = 0.0\n",
    "acuracia_classe2 = 0.0\n",
    "print(f'         acurácia,            std,                acurácia classe0,     acuracia clasee1')\n",
    "for train_index, test_index in kfold.split(X):\n",
    "  fold += 1\n",
    "  X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "\n",
    "  _W = np.zeros(X_train.shape[1])\n",
    "\n",
    "  # Treino\n",
    "  result_W = regressao_logistica(X_train, y_train, _W)\n",
    "  \n",
    "  # Teste\n",
    "  z_teste = X_test @ result_W\n",
    "  y_predito = sigmoide(z_teste)\n",
    "  \n",
    "\n",
    "  # obter a acurrácia\n",
    "  classes_preditas = [1 if i > 0.5 else 0 for i in y_predito]\n",
    "  classe1 = [i if i == 0 else None for i in classes_preditas]\n",
    "  classe2 = [i if i == 1 else None for i in classes_preditas]\n",
    "  acuracia += np.sum(y_test == classes_preditas)*1.00/len(y_test)\n",
    "  acuracia_classe1 += np.sum(y_test == classe1)*1.00/len(y_test)\n",
    "  acuracia_classe2 += np.sum(y_test == classe2)*1.00/len(y_test)\n",
    "  \n",
    "  print(f'Fold {fold} [{acuracia/10}, {np.std(classes_preditas, dtype=np.float64)}, {acuracia_classe1/10}, {acuracia_classe2/10}]')\n",
    "\n",
    "  # df_historico_k_custo = pd.DataFrame(data=custos, columns=['t','c'])\n",
    "  # fig, (ax1) = plt.subplots(1, 1)\n",
    "  # fig.suptitle(\"Gradiente Descendente\")\n",
    "  # fig.set_figwidth(12)\n",
    "  # fig.set_figheight(8)\n",
    "  # ax1.plot(df_historico_k_custo['t'], df_historico_k_custo['c'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2\n",
    "https://medium.com/mlearning-ai/multiclass-logistic-regression-with-python-2ee861d5772a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt(\"./vehicle.csv\", delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, :19]\n",
    "X = escalar_minmax.fit_transform(X)\n",
    "y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "alfa2 = 0.1\n",
    "epocas2 = 1000\n",
    "custos2 = []\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "def my_softmax2(z):\n",
    "  exp = np.exp(z)\n",
    "  return (exp / np.sum(exp))\n",
    "\n",
    "def custo_multi_cross_entropia(y, y_predi):\n",
    "    \"\"\"cross_entropy_loss\"\"\"\n",
    "    return -np.mean(np.sum(y * np.log(y_predi)))\n",
    "\n",
    "def acuracia(y, y_hat):\n",
    "    return np.sum(y == y_hat)/len(y)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "def treina(X, y):\n",
    "  _x = np.c_[np.ones(X.shape[0]), X]\n",
    "  y_encode = one_hot_encoder.fit_transform(y.reshape(-1,1))\n",
    "  W = np.zeros((_x.shape[1], y_encode.shape[1]))\n",
    "  \n",
    "  y_predi = None\n",
    "  n = X.shape[0]\n",
    "  for i in range(epocas2):\n",
    "    z = _x @ W\n",
    "    y_predi = softmax(z)\n",
    "    erro = y_encode - y_predi\n",
    "    \n",
    "    W = W + alfa2*((_x.T @ erro)/n)\n",
    "    custo2 = custo_multi_cross_entropia(y_encode, y_predi)\n",
    "    custos2.append([i, custo2])\n",
    "  return W\n",
    "   \n",
    "\n",
    "def testa(X, W):\n",
    "  features = np.c_[np.ones(X.shape[0]), X]\n",
    "  z = features @ W\n",
    "  y_predi_ = softmax(z)\n",
    "  return y_predi_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         acurácia,            std,                acurácia classe0,     acuracia clasee1\n",
      "Fold 10 [0.08823529411764705, 1.1642898841165181, 0.024705882352941178, 0.024705882352941178]\n",
      "Fold 10 [0.1764705882352941, 1.2621725653709506, 0.049411764705882356, 0.049411764705882356]\n",
      "Fold 10 [0.2752941176470588, 1.048313854585669, 0.07529411764705882, 0.07529411764705882]\n",
      "Fold 10 [0.36705882352941177, 1.1942190626305431, 0.09529411764705882, 0.09529411764705882]\n",
      "Fold 10 [0.46352941176470586, 1.1697454843981643, 0.13058823529411764, 0.13058823529411764]\n",
      "Fold 10 [0.5551960784313725, 1.239182215155736, 0.16511204481792716, 0.16511204481792716]\n",
      "Fold 10 [0.6468627450980392, 1.1639911011028874, 0.18773109243697478, 0.18773109243697478]\n",
      "Fold 10 [0.7373389355742297, 1.1050287034164088, 0.20558823529411763, 0.20558823529411763]\n",
      "Fold 10 [0.8278151260504203, 1.1842695348043701, 0.23535014005602237, 0.23535014005602237]\n",
      "Fold 10 [0.9218627450980392, 1.1368891796488878, 0.25796918767507, 0.25796918767507]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = KFold(n_splits=K, random_state=42, shuffle=True)\n",
    "acuracia_ = 0.0\n",
    "acuracia_classe_zero,  acuracia_classe_um, acuracia_classe_dois, acuracia_classe_tres= 0.0, 0.0, 0.0, 0.0\n",
    "print(f'         acurácia,            std,                acurácia classe0,     acuracia clasee1')\n",
    "for train_index, test_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "  \n",
    "  ws = treina(X_train, y_train)\n",
    "  # print(f'ws{ws}')\n",
    "  \n",
    "  y_ch = testa(X_test, ws)\n",
    "  classes_preditas = np.array([np.argmax(y)*1.0 for y in y_ch])\n",
    "\n",
    "  # obter a acurrácia\n",
    "  classe_zero= [i if i == 0 else None for i in classes_preditas]\n",
    "  classe_um = [i if i == 0 else None for i in classes_preditas]\n",
    "  acuracia_ += np.sum(y_test == classes_preditas)*1.00/len(y_test)\n",
    "  acuracia_classe_zero += np.sum(y_test == classe_zero)*1.00/len(y_test)\n",
    "  acuracia_classe_um += np.sum(y_test == classe_um)*1.00/len(y_test)\n",
    "\n",
    "  print(f'Fold {fold} [{acuracia_/K}, {np.std(classes_preditas, dtype=np.float64)}, {acuracia_classe_zero/K}, {acuracia_classe_um/K}]')\n",
    "\n",
    "# df_historico_multi_custo = pd.DataFrame(data=custos2, columns=['t','c'])\n",
    "# fig, (ax1) = plt.subplots(1, 1)\n",
    "# fig.suptitle(\"Custo Cross Entroty\")\n",
    "# fig.set_figwidth(12)\n",
    "# ax1.plot(df_historico_multi_custo['t'], df_historico_multi_custo['c'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MELHOR TUTORIAL\n",
    "https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
